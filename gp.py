import pandas as pd
import numpy as np
import os
import tempfile
from collections import Counter
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from lightgbm import LGBMClassifier, early_stopping
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from bayes_opt import BayesianOptimization
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectFromModel

# âœ… Windows í™˜ê²½ì—ì„œ joblib ì„ì‹œ í´ë” ì„¤ì • (ë©”ëª¨ë¦¬ ìµœì í™”)
temp_dir = tempfile.gettempdir()
os.environ["JOBLIB_TEMP_FOLDER"] = temp_dir

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
def load_and_preprocess():
    train = pd.read_csv('train.csv')
    test = pd.read_csv('test.csv')

    train.drop(columns=['ID'], errors='ignore', inplace=True)
    test.drop(columns=['ID'], errors='ignore', inplace=True)

    X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'], errors='ignore')
    y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']

    # âœ… Feature ì´ë¦„ í†µì¼ (ê³µë°± â†’ ì–¸ë”ë°”)
    X.columns = X.columns.str.replace(" ", "_")
    test.columns = test.columns.str.replace(" ", "_")

    # âœ… **ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê°•ì œ ë³€í™˜ (ì˜¤ë¥˜ ë°©ì§€)**
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
        test[col] = pd.to_numeric(test[col], errors='coerce')

    # âœ… **NaN ê°’ ì²˜ë¦¬ (ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ì›€)**
    X = X.fillna(X.median())
    test = test.fillna(test.median())

    # âœ… **Feature Engineering (ì—°ì‚° ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì¹˜í˜• ë³€í™˜ ì™„ë£Œ í›„ ì‹¤í–‰)**
    X['IVF_ì‹œìˆ _ë¹„ìœ¨'] = X['IVF_ì‹œìˆ _íšŸìˆ˜'] / (X['ì´_ì„ì‹ _íšŸìˆ˜'] + 1)
    X['DI_ì‹œìˆ _ë¹„ìœ¨'] = X['DI_ì‹œìˆ _íšŸìˆ˜'] / (X['ì´_ì„ì‹ _íšŸìˆ˜'] + 1)
    test['IVF_ì‹œìˆ _ë¹„ìœ¨'] = test['IVF_ì‹œìˆ _íšŸìˆ˜'] / (test['ì´_ì„ì‹ _íšŸìˆ˜'] + 1)
    test['DI_ì‹œìˆ _ë¹„ìœ¨'] = test['DI_ì‹œìˆ _íšŸìˆ˜'] / (test['ì´_ì„ì‹ _íšŸìˆ˜'] + 1)

    # âœ… Feature Selection ì ìš©
    selector = SelectFromModel(LGBMClassifier(n_estimators=100, random_state=42))
    selector.fit(X, y)
    selected_features = X.columns[selector.get_support()]
    X = selector.transform(X)

    # âœ… **Feature Selection í›„ test ë°ì´í„°ì—ë„ ë™ì¼í•œ Feature ì ìš©**
    selected_features = [f for f in selected_features if f in test.columns]
    test = test[selected_features]
    X = pd.DataFrame(X, columns=selected_features)
    test = pd.DataFrame(test, columns=selected_features)

    # âœ… **SMOTE ì ìš© (í´ë˜ìŠ¤ ê· í˜• ì¡°ì •)**
    smote = SMOTE(sampling_strategy=0.5, random_state=42)
    X, y = smote.fit_resample(X, y)

    # âœ… **ìŠ¤ì¼€ì¼ë§ ì ìš©**
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    test = scaler.transform(test)

    return X, y, test

# âœ… ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
X, y, test = load_and_preprocess()
print("âœ… ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì™„ë£Œ!")

# âœ… **Bayesian Optimizationì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**
def lgbm_eval(n_estimators, max_depth, learning_rate, subsample, colsample_bytree, lambda_l1, lambda_l2, num_leaves):
    model = LGBMClassifier(
        boosting_type='gbdt',
        objective='binary',
        random_state=42,
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        learning_rate=learning_rate,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        lambda_l1=lambda_l1,
        lambda_l2=lambda_l2,
        num_leaves=int(num_leaves),
        device='cpu',
        n_jobs=4  # âœ… ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€
    )

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    auc_scores = []

    for train_idx, valid_idx in skf.split(X, y):
        X_train, X_valid = X[train_idx], X[valid_idx]
        y_train, y_valid = y[train_idx], y[valid_idx]

        model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            eval_metric="auc",
            callbacks=[early_stopping(15, verbose=False)]
        )

        y_pred = model.predict_proba(X_valid)[:, 1]
        auc = roc_auc_score(y_valid, y_pred)
        auc_scores.append(auc)

    return np.mean(auc_scores)

# âœ… Bayesian Optimization ì„¤ì •
param_bounds = {
    'n_estimators': (200, 1000),
    'max_depth': (4, 12),
    'learning_rate': (0.005, 0.03),
    'subsample': (0.75, 1.0),
    'colsample_bytree': (0.75, 1.0),
    'lambda_l1': (0.01, 10),
    'lambda_l2': (0.01, 10),
    'num_leaves': (30, 150)
}

optimizer = BayesianOptimization(
    f=lgbm_eval,
    pbounds=param_bounds,
    random_state=42
)

optimizer.maximize(init_points=10, n_iter=50)

# âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©
best_params = optimizer.max['params']
best_params['n_estimators'] = int(best_params['n_estimators'])
best_params['max_depth'] = int(best_params['max_depth'])
best_params['num_leaves'] = int(best_params['num_leaves'])

# âœ… ëª¨ë¸ í•™ìŠµ (LightGBM + XGBoost + CatBoost)
best_lgb = LGBMClassifier(**best_params, boosting_type='gbdt', objective='binary', random_state=42, n_jobs=4)
best_xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=7, random_state=42, n_jobs=4)
best_cat = CatBoostClassifier(iterations=300, learning_rate=0.03, depth=6, random_state=42, verbose=0)

best_lgb.fit(X, y)
best_xgb.fit(X, y)
best_cat.fit(X, y)

# âœ… ì•™ìƒë¸” ì˜ˆì¸¡ (Stacking)
lgb_preds = best_lgb.predict_proba(test)[:, 1]
xgb_preds = best_xgb.predict_proba(test)[:, 1]
cat_preds = best_cat.predict_proba(test)[:, 1]

final_preds = (lgb_preds * 0.5) + (xgb_preds * 0.3) + (cat_preds * 0.2)

# âœ… ì œì¶œ íŒŒì¼ ì €ì¥
sample_submission = pd.read_csv('sample_submission.csv')
sample_submission['probability'] = final_preds
sample_submission.to_csv('final_submission.csv', index=False)

print(f"ğŸš€ ìµœì  ëª¨ë¸ AUC: {optimizer.max['target']:.4f}")
print("ğŸ¯ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. final_submission.csv ì €ì¥ ì™„ë£Œ.")
