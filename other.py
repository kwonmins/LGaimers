import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score

# âœ… ë°ì´í„° ë¡œë“œ
train = pd.read_csv('./train.csv').drop(columns=['ID'])
test = pd.read_csv('./test.csv').drop(columns=['ID'])

# âœ… 'ì„ì‹  ì„±ê³µ ì—¬ë¶€' ì»¬ëŸ¼ ì œê±° (Feature ê°œìˆ˜ ë§ì¶”ê¸°)
y = train["ì„ì‹  ì„±ê³µ ì—¬ë¶€"]
train.drop(columns=["ì„ì‹  ì„±ê³µ ì—¬ë¶€"], inplace=True)

# âœ… Feature ê°œìˆ˜ ê²€ì¦ (trainê³¼ testê°€ ë™ì¼í•´ì•¼ í•¨)
assert set(train.columns) == set(test.columns), "Feature ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# âœ… ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ (Label Encoding)
label_encoders = {}
for col in train.select_dtypes(include=["object"]).columns:
    le = LabelEncoder()
    combined_data = pd.concat([train[col], test[col]], axis=0)
    le.fit(combined_data.astype(str))
    train[col] = le.transform(train[col].astype(str))
    test[col] = le.transform(test[col].astype(str))
    label_encoders[col] = le

# âœ… NaN ê°’ ì²˜ë¦¬ (ìˆ˜ì¹˜í˜•ì€ 0ìœ¼ë¡œ, ë²”ì£¼í˜•ì€ ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ì›€)
num_imputer = SimpleImputer(strategy="constant", fill_value=0)
train.loc[:, train.select_dtypes(include=[np.number]).columns] = num_imputer.fit_transform(train.select_dtypes(include=[np.number]))
test.loc[:, test.select_dtypes(include=[np.number]).columns] = num_imputer.transform(test.select_dtypes(include=[np.number]))

# âœ… ğŸ”¥ ë²”ì£¼í˜• NaN ì²˜ë¦¬ (ğŸš¨ ì˜¤ë¥˜ í•´ê²°)
cat_columns = train.select_dtypes(include=["object"]).columns
if len(cat_columns) > 0:
    cat_imputer = SimpleImputer(strategy="most_frequent")
    train.loc[:, cat_columns] = cat_imputer.fit_transform(train[cat_columns])
    test.loc[:, cat_columns] = cat_imputer.transform(test[cat_columns])

# âœ… Feature Engineering (ë‹¤í•­ì‹ + ë¡œê·¸ ë³€í™˜ + ì œê³±ê·¼ ë³€í™˜)
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
train_poly = poly.fit_transform(train)
test_poly = poly.transform(test)

train_poly = pd.DataFrame(train_poly)
test_poly = pd.DataFrame(test_poly)

train_log = np.log1p(train + 1)
test_log = np.log1p(test + 1)

train_sqrt = np.sqrt(train + 1)
test_sqrt = np.sqrt(test + 1)

# âœ… ê¸°ì¡´ Feature + ìƒˆë¡œìš´ Feature í•©ì¹˜ê¸°
train = pd.concat([train, train_poly, train_log, train_sqrt], axis=1)
test = pd.concat([test, test_poly, test_log, test_sqrt], axis=1)

# âœ… Feature Selection (ë¶ˆí•„ìš”í•œ Feature ì œê±°)
selector = ExtraTreesClassifier(n_estimators=100, random_state=42)
selector.fit(train, y)

selected_features = train.columns[selector.feature_importances_ > 0.005]
train = train[selected_features]
test = test[selected_features]

# âœ… Feature ê°œìˆ˜ ê²€ì¦
assert train.shape[1] == test.shape[1], "Feature ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# âœ… LightGBM ëª¨ë¸ ì„¤ì •
lgb_params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 90,
    'learning_rate': 0.01,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_data_in_leaf': 25,
    'lambda_l1': 2.0,
    'lambda_l2': 4.0,
    'seed': 42
}

# âœ… Train/Validation Split
X_train, X_valid, y_train, y_valid = train_test_split(train, y, test_size=0.2, random_state=42, stratify=y)

# âœ… LightGBM ëª¨ë¸ í•™ìŠµ
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

lgb_model = lgb.train(
    lgb_params,
    train_data,
    valid_sets=[valid_data],
    num_boost_round=2000,
    callbacks=[lgb.early_stopping(stopping_rounds=100)]
)

# âœ… ìµœì¢… ì˜ˆì¸¡
pred_proba = lgb_model.predict(test)

# âœ… ê²°ê³¼ ì €ì¥
sample_submission = pd.read_csv('./sample_submission.csv')
sample_submission['probability'] = pred_proba
sample_submission.to_csv('./improved_submission.csv', index=False)
